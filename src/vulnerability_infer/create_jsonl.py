import json
import os
from collections import Counter
from pathlib import Path

import tiktoken
from dotenv import load_dotenv
from llm.prompt_formatter import PromptFormatter
from preprocessor.preprocessor import OWASPPreprocessor
from utils import parse_xml
from utils.data_utils import stratified_sample_list_of_dicts


# Initialize global variables and environment
def init_environment():
    ENV_FILE = Path(__file__).parent.parent / ".env"
    load_dotenv(ENV_FILE)
    REPO_ROOT = Path(__file__).parent.parent.parent
    os.makedirs(REPO_ROOT / ".data", exist_ok=True)
    return REPO_ROOT


REPO_ROOT = init_environment()

config = {
    "run_id": "test",
    "BenchmarkJavaPath": str(
        REPO_ROOT / "BenchmarkJava/src/main/java/org/owasp/benchmark/testcode"
    ),
    "BenchmarkJavaObfuscatedPaths": [
        (x, str(REPO_ROOT / ".data/obfs" / f"BenchmarkJavaObfuscated_v{x}"))
        for x in range(1, 5)
    ],
    "binary_classification_system_prompt": "simple_v2_binary_system_prompt",
    "binary_classification_prompt_user": "simple_binary_user_prompt",
    "cwe_detection_system_prompt": "vul_system_v2_prompt",
    "cwe_detection_user_prompt": "vul_user_prompt",
    #
    "stratify_keys": ["cwe_number", "category", "vulnerability"],
    "stratify_test_size": 0.1,
    "stratify_random_state": 12,
    "stratify_dataset": True,
}


prompt_formatter = PromptFormatter()
binary_messages_list = []
multi_messages_list = []

test_cases = []
for bid, BenchmarkJavaPath in config["BenchmarkJavaObfuscatedPaths"]:
    benchmark_test_cases = []
    for root, _, files in os.walk(BenchmarkJavaPath):
        for file in sorted(files):
            if file.endswith(".java"):
                test_case = {
                    "name": file,
                    "code": Path(root) / file,
                    "xml": Path(root) / (file.split(".")[0] + ".xml"),
                }
                test_case["info"] = parse_xml(test_case["xml"])
                benchmark_test_cases.append(test_case)

    if config["stratify_dataset"]:
        benchmark_test_cases = stratified_sample_list_of_dicts(
            benchmark_test_cases,
            config["stratify_keys"],
            config["stratify_test_size"],
            config["stratify_random_state"],
        )

    test_cases.extend(benchmark_test_cases)

print(Counter([x["info"]["cwe_number"] for x in test_cases]))
print(Counter([x["info"]["category"] for x in test_cases]))
print(Counter([x["info"]["vulnerability"] for x in test_cases]))

for test_no, test_case in enumerate(test_cases):
    binary_message = []
    multi_message = []
    with open(test_case["code"], "r", encoding="utf-8") as file:
        code = file.read()

    binary_prompt_system, binary_prompt_user = (
        prompt_formatter.process_system_chat_prompt(
            config["binary_classification_system_prompt"],
            config["binary_classification_prompt_user"],
            code,
        )
    )
    binary_expected = test_case["info"]["vulnerability"]

    binary_message.append(
        {
            "role": "system",
            "content": binary_prompt_system.content,
        }
    )

    # User message
    user_message_binary = {
        "role": "user",
        "content": binary_prompt_user.content,
    }
    binary_message.append(user_message_binary)

    # Assistant message
    assistant_message_binary = {
        "role": "assistant",
        "content": binary_expected,
    }
    binary_message.append(assistant_message_binary)

    # CWE detection
    multi_prompt_system, multi_prompt_user = (
        prompt_formatter.process_system_chat_prompt(
            config["cwe_detection_system_prompt"],
            config["cwe_detection_user_prompt"],
            code,
        )
    )
    cwe_expected = f"CWE-{test_case['info']['cwe_number']}"

    multi_message.append(
        {
            "role": "system",
            "content": multi_prompt_system.content,
        }
    )
    # User message
    user_message = {
        "role": "user",
        "content": multi_prompt_user.content,
    }
    multi_message.append(user_message)

    # Assistant message
    assistant_message = {
        "role": "assistant",
        "content": cwe_expected,
    }
    multi_message.append(assistant_message)

    binary_messages_list.append(
        json.dumps({"messages": binary_message}, separators=(",", ":"))
    )
    multi_messages_list.append(
        json.dumps({"messages": multi_message}, separators=(",", ":"))
    )

    # print progress
    print(f"Processed {test_no + 1}/{len(test_cases)} test cases", end="\r")


prices_per_token = {
    "gpt-3.5-turbo-0125": 0.000008,
}
encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")
number_of_tokens_3_5 = len(encoding.encode("".join(binary_messages_list)))

print(
    f"binary_finetuning_set, Number of tokens for model `gpt-3.5-turbo`: {number_of_tokens_3_5}"
    + f" Cost: {number_of_tokens_3_5 * prices_per_token['gpt-3.5-turbo-0125']:.5f}"
)

number_of_tokens_3_5 = len(encoding.encode("".join(multi_messages_list)))

print(
    f"multi_messages_list, Number of tokens for model `gpt-3.5-turbo`: {number_of_tokens_3_5}"
    + f" Cost: {number_of_tokens_3_5 * prices_per_token['gpt-3.5-turbo-0125']:.5f}"
)

# Write messages to the output file
with open(REPO_ROOT / ".data" / "binary_finetuning_set.jsonl", "w") as output:
    for _m in binary_messages_list:
        output.write(_m)
        output.write("\n")

# Write messages to the output file
with open(REPO_ROOT / ".data" / "multiclass_finetuning_set.jsonl", "w") as output:
    for test_no, _m in enumerate(multi_messages_list):
        output.write(_m)
        output.write("\n")
        print(f"Processed {test_no + 1}/{len(test_cases)} test cases", end="\r")
