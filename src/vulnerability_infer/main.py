import argparse
import concurrent.futures
import json
import os
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from copy import deepcopy
from datetime import datetime
from pathlib import Path
from random import randint

import utils
import utils.exceptions
from dotenv import load_dotenv
from llm.llm_runner import LLMRunner
from llm.prompt_formatter import PromptFormatter
from llm.response_formatter import ReponseFormatter
from utils import (
    calculate_classification_metrics,
    calculate_classification_metrics_binary,
    parse_xml,
    remove_comments,
)
from utils.data_utils import stratified_sample_list_of_dicts
from utils.setup_logging import setup_logging


# Initialize global variables and environment
def init_environment():
    ENV_FILE = Path(__file__).parent.parent / ".env"
    load_dotenv(ENV_FILE)
    REPO_ROOT = Path(__file__).parent.parent.parent
    os.makedirs(REPO_ROOT / ".data", exist_ok=True)
    return REPO_ROOT


logger = setup_logging("main")
logger.info("Application start")


def classify_test_case_binary_chat(
    test_case, config, cache_file_paths, llmRunner, avoid_rate_limiting=False
):
    try:
        system_prompt_id = config["binary_classification_system_prompt"]
        prompt_id = config["binary_classification_prompt_user"]

        cwe_system_prompt_id = config["cwe_detection_system_prompt"]
        cwe_prompt_id = config["cwe_detection_user_prompt"]

        if avoid_rate_limiting:
            # sleep for a random time between 1 and 5 seconds to avoid rate limiting
            time.sleep(randint(1, 3))

        prompt_start_time = time.time()
        logger.info(f"Classifying code: {test_case['name']}")
        # TODO
        if "full_code" in test_case:
            code = test_case["full_code"]
        else:
            with open(test_case["code"], "r") as file:
                code = remove_comments(file.read())

        prompt_formatter = PromptFormatter(show_token_count=True)
        binary_prompt = prompt_formatter.process_system_chat_prompt(
            system_prompt_id, prompt_id, code
        )

        binary_response = llmRunner.process_prompt(binary_prompt)
        is_vulnerable = ReponseFormatter.get_binary_vulnerability(binary_response)

        prompt = prompt_formatter.process_system_chat_prompt(
            cwe_system_prompt_id, cwe_prompt_id, code
        )
        response = llmRunner.process_prompt(prompt)
        cwe_number = ReponseFormatter.get_vulnerability(response)

        with open(cache_file_paths["response"], "a") as f:
            f.write(f"## {test_case['name']}\n")
            f.write(f"### Info\n```\n")
            f.write(f"CWE Number: {test_case['info']['cwe_number']}\n")
            f.write(f"Category: {test_case['info']['category']}\n")
            f.write(f"Test Number: {test_case['info']['test_number']}\n")
            f.write(f"Vulnerability: {test_case['info']['vulnerability']}\n```\n")
            f.write(f"\n\n### Prompt\n")
            f.write(
                f"\n{ReponseFormatter.make_md_collapsable('Prompt', ''.join([x.pretty_repr() for x in prompt]))}\n\n"
            )
            f.write(f"\n\n### Binary Prompt\n")
            f.write(
                f"\n{ReponseFormatter.make_md_collapsable('Prompt', ''.join([x.pretty_repr() for x in binary_prompt]))}\n\n"
            )

            f.write(f"### Response\n")
            f.write(
                f"Response: \n```\n{ReponseFormatter.escape_markdown(binary_response)}\n```\n\n"
            )
            f.write(
                f"Binary Parsed: {ReponseFormatter.get_binary_vulnerability(binary_response)}\n\n"
            )
            f.write(
                f"CWE Response: \n```\n{ReponseFormatter.escape_markdown(response)}\n```\n\n"
            )
            f.write(f"CWE Parsed: {cwe_number}\n\n")
            f.write(f"Prompt TimeTaken: {time.time()-prompt_start_time}\n\n")
            f.write(f"---\n\n")

    except Exception as e:
        logger.error(
            f"Command returned non-zero exit status: {e} for testcase: {config}"
        )
        response = "ERROR"

    return test_case["name"], (is_vulnerable, cwe_number)


def classify_test_case_binary(
    test_case, config, cache_file_paths, llmRunner, avoid_rate_limiting=False
):
    try:

        if avoid_rate_limiting:
            # sleep for a random time between 1 and 5 seconds to avoid rate limiting
            time.sleep(randint(1, 3))

        prompt_start_time = time.time()
        logger.info(f"Classifying code: {test_case['name']}")
        with open(test_case["code"], "r") as file:
            code = remove_comments(file.read())

        prompt_formatter = PromptFormatter()
        prompt = prompt_formatter.process_generic_prompt(
            config["binary_classification_prompt"], code
        )

        binary_response = llmRunner.process_prompt(prompt)
        is_vulnerable = ReponseFormatter.get_binary_vulnerability(binary_response)
        if is_vulnerable != "Unparseable":
            if is_vulnerable == "true":
                prompt = prompt_formatter.process_generic_prompt(
                    config["vulnerable_vulnerability_detection_prompt"], code
                )
                response = llmRunner.process_prompt(prompt)
            elif is_vulnerable == "false":
                prompt = prompt_formatter.process_generic_prompt(
                    config["fixed_vulnerability_detection_prompt"], code
                )
                response = llmRunner.process_prompt(prompt)
            else:
                response = "N/A"
            cwe_number = ReponseFormatter.get_vulnerability(response)
        else:
            response = "Unparseable"
            cwe_number = ReponseFormatter.get_vulnerability(response)

        with open(cache_file_paths["response"], "a") as f:
            f.write(f"## {test_case['name']}\n")
            f.write(f"### Info\n```\n")
            f.write(f"CWE Number: {test_case['info']['cwe_number']}\n")
            f.write(f"Category: {test_case['info']['category']}\n")
            f.write(f"Test Number: {test_case['info']['test_number']}\n")
            f.write(f"Vulnerability: {test_case['info']['vulnerability']}\n```\n")
            f.write(f"### Response\n")
            f.write(f"Binary Response: \n```\n{binary_response}\n```\n\n")
            f.write(
                f"Binary Parsed: {ReponseFormatter.get_binary_vulnerability(binary_response)}\n\n"
            )
            f.write(f"CWE Response: \n```\n{response}\n```\n\n")
            f.write(f"CWE Parsed: {cwe_number}\n\n")
            f.write(f"Prompt TimeTaken: {time.time()-prompt_start_time}\n\n")
            f.write(f"---\n\n")

    except Exception as e:
        logger.error(
            f"Command returned non-zero exit status: {e} for testcase: {config}"
        )
        response = "ERROR"

    return test_case["name"], (is_vulnerable, cwe_number)


def classify_test_case(
    test_case, config, cache_file_paths, llmRunner, avoid_rate_limiting=False
):
    try:

        if avoid_rate_limiting:
            # sleep for a random time between 1 and 5 seconds to avoid rate limiting
            time.sleep(randint(1, 5))

        prompt_start_time = time.time()
        logger.info(f"Classifying code: {test_case['name']}")
        with open(test_case["code"], "r") as file:
            code = remove_comments(file.read())

        prompt_formatter = PromptFormatter()
        prompt = prompt_formatter.process_generic_prompt(
            config["classification_prompt"], code
        )

        response = llmRunner.process_prompt(prompt)

        with open(cache_file_paths["response"], "a") as f:
            f.write(f"## {test_case['name']}\n")
            f.write(f"### Info\n```\n")
            f.write(f"CWE Number: {test_case['info']['cwe_number']}\n")
            f.write(f"Category: {test_case['info']['category']}\n")
            f.write(f"Test Number: {test_case['info']['test_number']}\n")
            f.write(f"Vulnerability: {test_case['info']['vulnerability']}\n```\n")
            f.write(f"### Response\n")
            f.write(f"Response: \n```\n{response}\n```\n\n")
            f.write(f"Parsed: {ReponseFormatter.get_vulnerability(response)}\n\n")
            f.write(f"Prompt TimeTaken: {time.time()-prompt_start_time}\n\n")
            f.write(f"---\n\n")

    except Exception as e:
        logger.error(
            f"Command returned non-zero exit status: {e} for testcase: {config}"
        )
        response = "ERROR"

    return test_case["name"], ReponseFormatter.get_vulnerability(response)


def classify_vulnerabilities(test_cases, config, cache_file_paths, model):
    """
    Classify vulnerabilities for a list of test cases using the specified model
    """
    if config["use_chat"]:
        classify_test_case_func = classify_test_case_binary_chat
    else:
        if config["classify_test_case_func"] == "binary_first_then_cwe":
            classify_test_case_func = classify_test_case_binary
        else:
            classify_test_case_func = classify_test_case

    try:
        llmRunner = LLMRunner(
            model=model,
            openai_key=config["openai_key"],
            ollama_url=config["ollama_url"],
            enable_chat=config["use_chat"],
        )
    except Exception as e:
        logger.error(f"Failed to initialize LLMRunner for model {model}: {e}")
        return None

    if model.startswith(("gpt-", "ft:gpt-")):
        with ThreadPoolExecutor(max_workers=4) as executor:
            future_to_test_case = {
                executor.submit(
                    classify_test_case_func,
                    test_case,
                    config,
                    cache_file_paths,
                    llmRunner,
                    True,
                ): test_case
                for test_case in test_cases
            }
            total_test_cases = len(test_cases)
            completed_test_cases = 0

            for future in as_completed(future_to_test_case):
                test_case_name, result = future.result()
                test_case = future_to_test_case[future]
                test_case["classification"] = result
                completed_test_cases += 1
                logger.info(
                    f"Processed {completed_test_cases}/{total_test_cases}: {test_case_name}"
                )
    else:
        total_test_cases = len(test_cases)
        for index, test_case in enumerate(test_cases, start=1):
            test_case["classification"] = classify_test_case_func(
                test_case, config, cache_file_paths, llmRunner
            )[1]
            logger.info(f"Processed {index}/{total_test_cases}: {test_case['name']}")

    return test_cases


def get_cache_file_paths(repo_root, config, model):
    cache_file_paths = {
        "classification": f"{repo_root}/.data/{config['run_id']}/{model}/classification.json",
        "response": f"{repo_root}/.data/{config['run_id']}/{model}/response.md",
        "output": f"{repo_root}/.data/{config['run_id']}/{model}/output.csv",
        "owasp_output_format": f"{repo_root}/.data/{config['run_id']}/{model}/owasp_output_format.csv",
        "analysis_results": f"{repo_root}/.data/{config['run_id']}/{model}/analysis_results.json",
        "analysis_results_binary": f"{repo_root}/.data/{config['run_id']}/{model}/analysis_results_binary.json",
    }

    os.makedirs(f"{repo_root}/.data/{config['run_id']}/{model}", exist_ok=True)
    return cache_file_paths


# Main function to run the application
def main(run_id, run_on_obfuscated):
    REPO_ROOT = init_environment()

    config = {
        "run_id": run_id,
        "run_on_obfuscated": run_on_obfuscated,
        "ollama_url": os.getenv("OLLAMA_URL"),
        "openai_key": os.getenv("OPENAI_API_KEY"),
        # "classification_models": ["llama3:70b"],
        "classification_models": [
            "gpt-4-0125-preview",  # preferred model
            # "ft:gpt-3.5-turbo-0125:personal:binary-secdev:9Ngmdwsa",
            # "ft:gpt-3.5-turbo-0125:personal:owasp-secdev:9NSiX5NY",
            # "gpt-3.5-turbo",
            # "llama3:70b",
            # "llama2:70b",
            # "mistral:instruct",
            # "mixtral:8x22b-instruct",
            # "gemma:7b-instruct",
            # "codegemma:instruct",
            # "codellama:34b-instruct",
            # "deepseek-coder:33b",
            ############################################
            # "mistral:7b-instruct-v0.2-fp16",
            # "gpt-4-1106-preview",
            # "codellama:13b-instruct",
            # "deepseek-coder:33b",
            # "dolphin-mixtral:8x7b",
            # "gemma:2b-instruct-fp16",
            # "mixtral:8x7b",
            # "phind-codellama:34b-v2",
        ],
        "use_chat": True,
        "binary_classification_system_prompt": "simple_v2_binary_system_prompt",
        "binary_classification_prompt_user": "simple_binary_user_prompt",
        "cwe_detection_system_prompt": "vul_system_v2_prompt",
        "cwe_detection_user_prompt": "vul_user_prompt",
        #
        "classification_prompt": "init_vulnerability_detection_prompt_2",
        "binary_classification_prompt": "binary_vulnerability_detection_prompt",
        "fixed_vulnerability_detection_prompt": "vulnerable_vulnerability_detection_prompt",
        "vulnerable_vulnerability_detection_prompt": "vulnerable_vulnerability_detection_prompt",
        "request_timeout": 60,
        "BenchmarkJavaPath": str(
            REPO_ROOT / "BenchmarkJava/src/main/java/org/owasp/benchmark/testcode"
        ),
        "BenchmarkJavaObfuscatedPath": str(REPO_ROOT / "BenchmarkJavaObfuscated"),
        "stratify_keys": ["cwe_number", "category"],
        "stratify_test_size": 0.05,
        "stratify_random_state": 12,
        "stratify_dataset": False,
        "classify_test_case_func": "binary_first_then_cwe",
        #
        "run_realworld": False,
        "real_world_dataset": str(REPO_ROOT / "realworld_data/real_world_dataset.json"),
    }

    # configure model specific parameters
    # TODO: Curretly not used
    config["classification_prompts"] = {
        "codellama:13b-instruct": "init_vulnerability_detection_prompt",
        "codellama:34b-instruct": "init_vulnerability_detection_prompt",
        "deepseek-coder:33b": "init_vulnerability_detection_prompt",
        "dolphin-mixtral:8x7b": "init_vulnerability_detection_prompt",
        "gemma:2b-instruct-fp16": "init_vulnerability_detection_prompt",
        "gemma:7b-instruct-fp16": "init_vulnerability_detection_prompt",
        "llama2:70b": "init_vulnerability_detection_prompt",
        "llama3:70b": "init_vulnerability_detection_prompt",
        "mistral:7b-instruct-v0.2-fp16": "init_vulnerability_detection_prompt",
        "mistral:instruct": "init_vulnerability_detection_prompt",
        "mixtral:8x7b": "init_vulnerability_detection_prompt",
        "mixtral:8x22b-instruct": "init_vulnerability_detection_prompt",
        "phind-codellama:34b-v2": "init_vulnerability_detection_prompt",
    }

    logger.info("Starting vulnerability classification")
    logger.info(f"Configuration: {json.dumps(config, indent=4)}")

    if config["run_on_obfuscated"]:
        database_path = config["BenchmarkJavaObfuscatedPath"]
    else:
        database_path = config["BenchmarkJavaPath"]

    filter_test_cases = []
    try:
        with open(f"{REPO_ROOT}/.data/filter_testcases.txt", "r") as file:
            # strip the newline character from each line
            filter_test_cases = [line.strip() for line in file.readlines()]
    except FileNotFoundError:
        # Handle the case when the file is not found
        print("Filter test cases file not found.")

    test_cases = []
    for root, _, files in os.walk(database_path):
        for file in sorted(files):
            if filter_test_cases and file not in filter_test_cases:
                continue
            if file.endswith(".java"):
                test_case = {
                    "name": file,
                    "code": Path(root) / file,
                    "xml": Path(root) / (file.split(".")[0] + ".xml"),
                }
                test_case["info"] = parse_xml(test_case["xml"])
                test_cases.append(test_case)

    if config["stratify_dataset"]:
        test_cases = stratified_sample_list_of_dicts(
            test_cases,
            config["stratify_keys"],
            config["stratify_test_size"],
            config["stratify_random_state"],
        )

    # Prepare realworld
    if config["run_realworld"]:
        with open(config["real_world_dataset"], "r") as file:
            test_cases = json.load(file)

    analysis_results = {}
    analysis_results_binary = {}
    # classify vulnerabilities for each model in the classification_models list and store the results separately
    for model in config["classification_models"]:
        cache_file_paths = get_cache_file_paths(REPO_ROOT, config, model)

        # check if the model is already classified using the cache file for analysis results
        if os.path.exists(cache_file_paths["analysis_results"]):
            with open(cache_file_paths["analysis_results"], "r") as f:
                analysis_results[model] = json.load(f)
            with open(cache_file_paths["analysis_results_binary"], "r") as f:
                analysis_results_binary[model] = json.load(f)
            continue

        model_start_time = time.time()
        test_cases_results = deepcopy(test_cases)
        test_cases_results = classify_vulnerabilities(
            test_cases_results, config, cache_file_paths, model
        )

        if not test_cases_results:
            logger.error(f"No response found for model {model}")
            continue

        # store dict test_cases as a csv with expected cwe and predicted cwe
        if config["classify_test_case_func"] == "binary_first_then_cwe":
            with open(cache_file_paths["output"], "w") as f:
                f.write(
                    "Test Name,Expected is Vulnerable,Expected CWE,Predicted is Vulnerable,Predicted CWE\n"
                )
                for test_case in test_cases_results:
                    f.write(
                        f"{test_case['name']},{test_case['info']['vulnerability']},{test_case['info']['cwe_number']},{test_case['classification'][0]},{test_case['classification'][1]}\n"
                    )

            with open(cache_file_paths["owasp_output_format"], "w") as f:
                f.write(
                    f"test name,category,CWE,real vulnerability,identified by tool,pass/fail,Benchmark version: 1.2, Actual results generated: {datetime.today().strftime('%Y-%m-%d')}\n"
                )
                for test_case in test_cases_results:
                    f.write(
                        f"{test_case['name']},{test_case['info']['category']},{test_case['info']['cwe_number']},{test_case['info']['vulnerability']},{test_case['classification'][0]},{utils.check_owasp_test_pass(test_case)}\n"
                    )

            logger.info(
                f"Model {model} took {time.time() - model_start_time} seconds to classify vulnerabilities"
            )
            analysis_result_binary = calculate_classification_metrics_binary(
                cache_file_paths["output"]
            )
            analysis_results_binary[model] = analysis_result_binary
            # store analysis results as a json file
            with open(cache_file_paths["analysis_results_binary"], "w") as f:
                f.write(json.dumps(analysis_result_binary, indent=4))

            analysis_result = calculate_classification_metrics(
                cache_file_paths["output"]
            )
            analysis_results[model] = analysis_result
            # store analysis results as a json file
            with open(cache_file_paths["analysis_results"], "w") as f:
                f.write(json.dumps(analysis_result, indent=4))

            logger.info(
                f"Analysis results binary: {json.dumps(analysis_result_binary, indent=4)}"
            )
            logger.info(f"Analysis results: {json.dumps(analysis_result, indent=4)}")
        else:
            with open(cache_file_paths["output"], "w") as f:
                f.write("Test Name,Expected CWE,Predicted CWE\n")
                for test_case in test_cases_results:
                    f.write(
                        f"{test_case['name']},{test_case['info']['cwe_number'] if test_case['info']['vulnerability'] == 'true' else 00},{test_case['classification']}\n"
                    )

            logger.info(
                f"Model {model} took {time.time() - model_start_time} seconds to classify vulnerabilities"
            )
            analysis_result = calculate_classification_metrics(
                cache_file_paths["output"]
            )
            analysis_results[model] = analysis_result
            # store analysis results as a json file
            with open(cache_file_paths["analysis_results"], "w") as f:
                f.write(json.dumps(analysis_result, indent=4))

            logger.info(f"Analysis results: {json.dumps(analysis_result, indent=4)}")

    # convert analysis results accuracy precision recall f1 of all models from analysis_results dict into a csv file
    with open(f"{REPO_ROOT}/.data/{config['run_id']}/analysis_results.csv", "w") as f:
        f.write("Model,Accuracy,Precision,Recall,F1,F1 Weighted,MCC\n")
        for model, result in analysis_results.items():
            f.write(
                f"{model},{result['accuracy']},{result['precision']},{result['recall']},{result['f1']},{result['f1_weighted']},{result['mcc']}\n"
            )

    if config["classify_test_case_func"] == "binary_first_then_cwe":
        # convert analysis results accuracy precision recall f1 of all models from analysis_results dict into a csv file
        with open(
            f"{REPO_ROOT}/.data/{config['run_id']}/analysis_results_binary.csv", "w"
        ) as f:
            f.write("Model,Accuracy,Precision,Recall,F1,MCC\n")
            for model, result in analysis_results_binary.items():
                f.write(
                    f"{model},{result['accuracy']},{result['precision']},{result['recall']},{result['f1']},{result['mcc']}\n"
                )


if __name__ == "__main__":
    start_time = time.time()
    # TODO: Add argparse to pass the configuration file
    default_run_id = "Rerun_failed_tests_gpt4"

    parser = argparse.ArgumentParser()

    parser.add_argument(
        "--run_id",
        type=str,
        help="Run ID for the current run",
        default=default_run_id,
    )
    parser.add_argument(
        "--run_on_obfuscated",
        type=bool,
        help="Run on obfuscated code",
        default=False,
    )

    args = parser.parse_args()

    main(args.run_id, False)

    if args.run_on_obfuscated:
        logger.info("Running on obfuscated code")
        main(f"{args.run_id}_obfuscated", True)

    logger.info(f"Total time taken: {time.time() - start_time} seconds")
