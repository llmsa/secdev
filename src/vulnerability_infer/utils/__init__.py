import json
import logging
import re
import xml.etree.ElementTree as ET
from pathlib import Path

import pandas as pd
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix,
    f1_score,
    matthews_corrcoef,
    precision_score,
    recall_score,
)

logger = logging.getLogger(f"main.{__name__}")


def parse_xml(xml_file_path):
    try:
        tree = ET.parse(xml_file_path)
        root = tree.getroot()
        # Extract the CWE number from the XML
        cwe_number = root.find("cwe").text
        category = root.find("category").text
        test_number = root.find("test-number").text
        vulnerability = root.find("vulnerability").text

        test_case_info = {
            "cwe_number": cwe_number,
            "category": category,
            "test_number": test_number,
            "vulnerability": vulnerability,
        }
    except Exception as e:
        logger.error(f"Error extracting CWE number from XML: {e}")
        test_case_info = {
            "cwe_number": "N/A",
            "category": "N/A",
            "test_number": "N/A",
            "vulnerability": "N/A",
        }

    return test_case_info


def remove_comments(string):
    pattern = r"(\".*?\"|\'.*?\')|(/\*.*?\*/|//[^\r\n]*$)"
    # first group captures quoted strings (double or single)
    # second group captures comments (//single-line or /* multi-line */)
    regex = re.compile(pattern, re.MULTILINE | re.DOTALL)

    def _replacer(match):
        # if the 2nd group (capturing comments) is not None,
        # it means we have captured a non-quoted (real) comment string.
        if match.group(2) is not None:
            return ""  # so we will return empty to remove the comment
        else:  # otherwise, we will return the 1st group
            return match.group(1)  # captured quoted-string

    return regex.sub(_replacer, string)


def calculate_classification_metrics(csv_path):
    try:
        # Load data from CSV
        df = pd.read_csv(csv_path)

        # Assuming the CSV has columns named 'Expected' and 'Predicted'
        y_true = df["Expected CWE"]
        y_pred = df["Predicted CWE"]

        # Calculate various metrics
        accuracy = accuracy_score(y_true, y_pred)
        precision = precision_score(y_true, y_pred, average="macro")
        recall = recall_score(y_true, y_pred, average="macro")
        f1 = f1_score(y_true, y_pred, average="macro")
        conf_matrix = confusion_matrix(y_true, y_pred)

        # Calculate various metrics
        accuracy = accuracy_score(y_true, y_pred)
        f1_weighted = f1_score(y_true, y_pred, average="weighted")
        mcc = matthews_corrcoef(y_true, y_pred)

        # return the calculated metrics as a dict
        return {
            "accuracy": accuracy,
            "precision": precision,
            "recall": recall,
            "f1": f1,
            "f1_weighted": f1_weighted,
            "mcc": mcc,
        }
    except Exception as e:
        logger.error(f"Error calculating classification metrics: {e}")
        return {
            "accuracy": 0,
            "precision": 0,
            "recall": 0,
            "f1": 0,
            "f1_weighted": 0,
            "mcc": 0,
        }


def calculate_classification_metrics_binary(csv_path):
    try:
        # Load data from CSV
        df = pd.read_csv(csv_path)

        # Assuming the CSV has columns named 'Expected' and 'Predicted'
        y_true = df["Expected is Vulnerable"]
        y_pred = df["Predicted is Vulnerable"]

        # Calculate various metrics
        accuracy = accuracy_score(y_true, y_pred)
        precision = precision_score(y_true, y_pred, average="macro")
        recall = recall_score(y_true, y_pred, average="macro")
        f1 = f1_score(y_true, y_pred, average="macro")
        conf_matrix = confusion_matrix(y_true, y_pred)
        mcc = matthews_corrcoef(y_true, y_pred)

        # return the calculated metrics as a dict
        return {
            "accuracy": accuracy,
            "precision": precision,
            "recall": recall,
            "f1": f1,
            "mcc": mcc,
        }
    except Exception as e:
        logger.error(f"Error calculating classification metrics: {e}")
        return {
            "accuracy": 0,
            "precision": 0,
            "recall": 0,
            "f1": 0,
            "mcc": 0,
        }


def check_owasp_test_pass(test_case):
    if (test_case["info"]["cwe_number"] == test_case["classification"][1]) and (
        test_case["info"]["vulnerability"] == test_case["classification"][0]
    ):
        return "true"
    else:
        return "false"
